{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertCnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mKOKiN1RsoUtuUIbfP5tiuR3N_dFL-Du",
      "authorship_tag": "ABX9TyMifdwfAf6ul4CwbOFYpqYK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asabbah44/Text_Classification/blob/main/BertCnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byicQrOsc-Z2"
      },
      "source": [
        "https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD7Vxps7va25",
        "outputId": "1115615d-8e2b-4ff8-9e08-0ef64bdf365c"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 370,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.7/dist-packages (0.14.9)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.10.2)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19ganABlvt66"
      },
      "source": [
        "try:\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOvlZD0iwzC6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18NtJbsev1IC"
      },
      "source": [
        "df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/dataset.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/mydataset.csv')\n",
        "\n",
        "\n",
        "df1 = pd.DataFrame(df1, columns = ['commenttext','label'])\n",
        "df2 = pd.DataFrame(df2, columns = ['commenttext','label'])\n",
        "\n",
        "df= df2 #pd.concat([df1,df2])\n",
        "\n"
      ],
      "execution_count": 485,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65e4gq-CwctR",
        "outputId": "ff91e8cd-215c-48ee-d3e9-9a44c5678cb8"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 486,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1513, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 486
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af3kBujzxHNy"
      },
      "source": [
        "def preprocess_text(sen):\n",
        "    # Removing html tags\n",
        "    sentence = remove_tags(sen)\n",
        "\n",
        "    # Remove punctuations and numbers\n",
        "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
        "\n",
        "    # Single character removal\n",
        "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
        "\n",
        "    # Removing multiple spaces\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
        "\n",
        "    return sentence"
      ],
      "execution_count": 487,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wfpyA9nxIqS"
      },
      "source": [
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(text):\n",
        "    return TAG_RE.sub('', text)"
      ],
      "execution_count": 488,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMZirreZxLFB"
      },
      "source": [
        "comments= []\n",
        "sentences = list(df['commenttext'])\n",
        "for sen in sentences:\n",
        "    comments.append(preprocess_text(sen))"
      ],
      "execution_count": 489,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frtm1EHqxj3y",
        "outputId": "ffe7d182-0b42-4c9f-b705-160c45549d49"
      },
      "source": [
        "df.label.unique()"
      ],
      "execution_count": 490,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Documentation', 'Design', 'Test', 'Requirement', 'Defect'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 490
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7BFG58uyF0K"
      },
      "source": [
        "y = df['label']\n",
        "\n",
        "y = np.array(list(map(lambda x: 0 if x==\"Requirement\" else 1 if x==\"Design\" else 2 if x==\"Defect\" else 3 if x==\"Test\" else 4, y)))"
      ],
      "execution_count": 491,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJhcAYjvyKlx"
      },
      "source": [
        "Creating a BERT Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZToqNkh9yLNg"
      },
      "source": [
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\",\n",
        "                            trainable=False)\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": 492,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKdOECbhyzSK",
        "outputId": "b8cdeb2f-8b60-45eb-d016-7e74feda357f"
      },
      "source": [
        "tokenizer.tokenize(\"don't need this classificatio\")"
      ],
      "execution_count": 493,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['don', \"'\", 't', 'need', 'this', 'class', '##ific', '##ati', '##o']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 493
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c42dKzBNy71S",
        "outputId": "ee77d7b5-86cf-474a-f783-672a130aed9a"
      },
      "source": [
        "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"don't need this class\"))"
      ],
      "execution_count": 494,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2123, 1005, 1056, 2342, 2023, 2465]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 494
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaoKzZfo2Wfi"
      },
      "source": [
        "def tokenize_comments(text_comments):\n",
        "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_comments))"
      ],
      "execution_count": 495,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-puzyQFg2gxq"
      },
      "source": [
        "tokenize_comments = [tokenize_comments(comment) for comment in comments]"
      ],
      "execution_count": 464,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksiYLVR93LIJ"
      },
      "source": [
        "Prerparing Data For Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXnUlxzg3MNx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "63c19fbf-3894-4132-b53c-9a53542fc89d"
      },
      "source": [
        "comments_with_len = [[comment, y[i], len(comment)]\n",
        "                 for i, comment in enumerate(tokenize_comments)]"
      ],
      "execution_count": 496,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-496-8736342a212d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m comments_with_len = [[comment, y[i], len(comment)]\n\u001b[0;32m----> 2\u001b[0;31m                  for i, comment in enumerate(tokenize_comments)]\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'function' object is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqh1_wKn8E-8"
      },
      "source": [
        "print(comments_with_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbXPmUtv31iC"
      },
      "source": [
        "import random\n",
        "random.shuffle(comments_with_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjcVSJzJ_77q"
      },
      "source": [
        "print(comments_with_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Fi-vNH36Sa"
      },
      "source": [
        "# sort by the len of comments\n",
        "#comments_with_len.sort(key=lambda x: x[2])"
      ],
      "execution_count": 469,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhZkRSsR4LDl"
      },
      "source": [
        "remove the length attribute from all the comments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI-dqL0H4NuB"
      },
      "source": [
        "sorted_comments_labels = [(comment_lab[0], comment_lab[1]) for comment_lab in comments_with_len]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwuUnOYyAsuL"
      },
      "source": [
        "# only comment and label sorted \n",
        "print(sorted_comments_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhDax3j94sba"
      },
      "source": [
        "convert the sorted dataset into a TensorFlow 2.0-compliant input dataset shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4fQrW1a4sua"
      },
      "source": [
        "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_comments_labels, output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yw6-cd63zNus"
      },
      "source": [
        "processed_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9XMFFHR47x6"
      },
      "source": [
        " Pad our dataset for each batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_odtwrh49MT"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR-7bXXc5B97"
      },
      "source": [
        "next(iter(batched_dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWn01-IJCQCj"
      },
      "source": [
        "divide the dataset into test and training sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsoWCkdZCQu6"
      },
      "source": [
        "import math\n",
        "TOTAL_BATCHES = math.ceil(len(sorted_comments_labels) / BATCH_SIZE)\n",
        "TEST_BATCHES = TOTAL_BATCHES // 10  # 5= 20% , 10= 10%\n",
        "batched_dataset.shuffle(TOTAL_BATCHES)\n",
        "test_data = batched_dataset.take(TEST_BATCHES)\n",
        "train_data = batched_dataset.skip(TEST_BATCHES)\n",
        "\n",
        "                                                            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3MItOmrL1_B"
      },
      "source": [
        "TOTAL_BATCHES\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDYwJOelCoab"
      },
      "source": [
        "Creating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bfl0agpUCq9T"
      },
      "source": [
        "class TEXT_MODEL(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self,\n",
        "                 vocabulary_size,\n",
        "                 embedding_dimensions=128,\n",
        "                 cnn_filters=50,\n",
        "                 dnn_units=512,\n",
        "                 model_output_classes=5,\n",
        "                 dropout_rate=0.1,\n",
        "                 training=False,\n",
        "                \n",
        "                 name=\"text_model\"):\n",
        "        super(TEXT_MODEL, self).__init__(name=name)\n",
        "        \n",
        "        self.embedding = layers.Embedding(vocabulary_size,\n",
        "                                          embedding_dimensions,input_length=512)\n",
        "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=2,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=3,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=4,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.cnn_layer4 = layers.Conv1D(filters=cnn_filters,\n",
        "                                        kernel_size=5,\n",
        "                                        padding=\"valid\",\n",
        "                                        activation=\"relu\")\n",
        "        self.pool = layers.GlobalMaxPool1D()\n",
        "        \n",
        "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "        if model_output_classes == 2:\n",
        "            self.last_dense = layers.Dense(units=1,\n",
        "                                           activation=\"sigmoid\")\n",
        "        else:\n",
        "            self.last_dense = layers.Dense(units=model_output_classes,\n",
        "                                           activation=\"softmax\")\n",
        "    \n",
        "    def call(self, inputs, training):\n",
        "        l = self.embedding(inputs)\n",
        "        l_1 = self.cnn_layer1(l) \n",
        "        l_1 = self.pool(l_1) \n",
        "        l_2 = self.cnn_layer2(l) \n",
        "        l_2 = self.pool(l_2)\n",
        "        # l_3 = self.cnn_layer3(l)\n",
        "        # l_3 = self.pool(l_3) \n",
        "        # l_4 = self.cnn_layer4(l)\n",
        "        # l_4 = self.pool(l_4) \n",
        "        \n",
        "        concatenated = tf.concat([l_1, l_2], axis=-1) # (self.batch_size, 3 * self.cnn_filters)\n",
        "        concatenated = self.dense_1(concatenated)\n",
        "        concatenated = self.dropout(concatenated, training)\n",
        "        model_output = self.last_dense(concatenated)\n",
        "        \n",
        "        return model_output"
      ],
      "execution_count": 497,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9TdjOxsDDKi"
      },
      "source": [
        "VOCAB_LENGTH = len(tokenizer.vocab)\n",
        "EMB_DIM = 50\n",
        "CNN_FILTERS = 100\n",
        "DNN_UNITS = 512\n",
        "OUTPUT_CLASSES = 5\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": 498,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPY6DzetDGWS"
      },
      "source": [
        "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
        "                        embedding_dimensions=EMB_DIM,\n",
        "                        cnn_filters=CNN_FILTERS,\n",
        "                        dnn_units=DNN_UNITS,\n",
        "                        model_output_classes=OUTPUT_CLASSES,\n",
        "                        dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": 499,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgAWUkQWDLUa"
      },
      "source": [
        "if OUTPUT_CLASSES == 2:\n",
        "    text_model.compile(loss=\"binary_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"accuracy\"])\n",
        "else:\n",
        "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": 500,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1zKODEpDNwS",
        "outputId": "7bdd255a-c7f4-4cd3-85a5-52dd738afd88"
      },
      "source": [
        "text_model.fit(train_data, epochs=NB_EPOCHS)"
      ],
      "execution_count": 501,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "116/116 [==============================] - 5s 38ms/step - loss: 1.0784 - sparse_categorical_accuracy: 0.6454\n",
            "Epoch 2/5\n",
            "116/116 [==============================] - 5s 38ms/step - loss: 0.7564 - sparse_categorical_accuracy: 0.7366\n",
            "Epoch 3/5\n",
            "116/116 [==============================] - 5s 38ms/step - loss: 0.4995 - sparse_categorical_accuracy: 0.8430\n",
            "Epoch 4/5\n",
            "116/116 [==============================] - 5s 38ms/step - loss: 0.2922 - sparse_categorical_accuracy: 0.9103\n",
            "Epoch 5/5\n",
            "116/116 [==============================] - 5s 38ms/step - loss: 0.1523 - sparse_categorical_accuracy: 0.9543\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa9078ecf50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 501
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVMgtwQ1DQ-C",
        "outputId": "21c4dd6f-9c10-493d-a568-66a58a1b56a1"
      },
      "source": [
        "results = text_model.evaluate(test_data)\n",
        "print(results)"
      ],
      "execution_count": 502,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/12 [==============================] - 0s 14ms/step - loss: 0.9842 - sparse_categorical_accuracy: 0.8125\n",
            "[0.9841931462287903, 0.8125]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9zjyA9IeOQT",
        "outputId": "f50acf31-96ee-489b-921a-5770695c8028"
      },
      "source": [
        "results = text_model.evaluate(train_data)\n",
        "print(results)"
      ],
      "execution_count": 503,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "116/116 [==============================] - 2s 15ms/step - loss: 0.1076 - sparse_categorical_accuracy: 0.9639\n",
            "[0.10762815922498703, 0.9639273285865784]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}