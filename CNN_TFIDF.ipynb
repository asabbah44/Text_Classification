{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_TFIDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fiurN3XMH5yORtg2a8GYo6D2e9oxj7Ve",
      "authorship_tag": "ABX9TyN04anl0PjUuuwMPYhTjkxt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asabbah44/Text_Classification/blob/main/CNN_TFIDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PaWeHP4fUcK"
      },
      "source": [
        "https://hub.packtpub.com/classify-emails-using-deep-neural-networks-generating-tf-idf/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro-qQOCziaAh"
      },
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import string\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXB0K-jrSFQp",
        "outputId": "64b96f60-dc9e-4bbe-c4d6-52b7a348870c"
      },
      "source": [
        "\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/dataset.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/mydatasetfinal.csv')\n",
        "\n",
        "\n",
        "df1 = pd.DataFrame(df1, columns = ['commenttext','label'])\n",
        "df2 = pd.DataFrame(df2, columns = ['commenttext','label'])\n",
        "df=pd.concat([df1,df2])\n",
        "\n",
        "\n",
        "y = df['label']\n",
        "y = np.array(list(map(lambda x: 0 if x==\"Requirement\" else 1 if x==\"Design\" else 2 if x==\"Defect\" else 3 if x==\"Test\" else 4, y)))\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.commenttext, y, test_size=0.20, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "x_train = X_train\n",
        "\n",
        "x_test = X_test\n",
        "\n",
        "y_train = y_train\n",
        "\n",
        "y_test = y_test\n",
        "\n",
        "\n",
        "\n",
        "print (x_train[0])\n",
        "\n",
        "print (\"Sample Target Category:\")\n",
        "\n",
        "print(len(x_train))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "// FIXME formatters are not thread-safe\n",
            "Sample Target Category:\n",
            "4467\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz4hrs1HQupI"
      },
      "source": [
        "def preprocessing(text):\n",
        "  text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
        "  tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]\n",
        "  tokens = [word.lower() for word in tokens]\n",
        "  stopwds = stopwords.words('english')\n",
        "  tokens = [token for token in tokens if token not in stopwds]\n",
        "  tokens = [word for word in tokens if len(word)>=3]\n",
        "  tagged_corpus = pos_tag(tokens)\n",
        "  Noun_tags = ['NN','NNP','NNPS','NNS']\n",
        "  Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  def prat_lemmatize(token,tag):\n",
        "\n",
        "   if tag in Noun_tags:\n",
        "\n",
        "    return lemmatizer.lemmatize(token,'n')\n",
        "\n",
        "   elif tag in Verb_tags:\n",
        "\n",
        "    return lemmatizer.lemmatize(token,'v')\n",
        "\n",
        "   else:\n",
        "\n",
        "    return lemmatizer.lemmatize(token,'n')\n",
        "  pre_proc_text = \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])\n",
        "\n",
        "  return pre_proc_text"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9KXt6Z7UXcA",
        "outputId": "7f046339-bd9f-406b-a9ec-a62170178d81"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "x_train_preprocessed = []\n",
        "\n",
        "\n",
        "for i in x_train:\n",
        "\n",
        " x_train_preprocessed.append(preprocessing(i))\n",
        "\n",
        " x_test_preprocessed = []\n",
        "\n",
        "for i in x_test:\n",
        "\n",
        " x_test_preprocessed.append(preprocessing(i))\n",
        "\n",
        "# building TFIDF vectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2), stop_words='english', max_features= 10000,strip_accents='unicode', norm='l2')\n",
        "\n",
        "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()\n",
        "\n",
        "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWl5WYhCVJhg"
      },
      "source": [
        " #Deep Learning modules\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers.core import Dense, Dropout, Activation\n",
        "\n",
        "from keras.optimizers import Adadelta,Adam,RMSprop\n",
        "\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc1_8E9JVPJo"
      },
      "source": [
        "# Definition hyper parameters\n",
        "\n",
        "np.random.seed(1337)\n",
        "\n",
        "nb_classes = 5\n",
        "\n",
        "batch_size =32\n",
        "\n",
        "nb_epochs = 10"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGH6Ch1yXp1p",
        "outputId": "f164a64e-cecf-41e8-d362-e6fffbe52f92"
      },
      "source": [
        "x_train_2.shape[1]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7319"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9rZDT9PVTZI"
      },
      "source": [
        "Y_train = np_utils.to_categorical(y_train, nb_classes)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoH43YfuZ65I",
        "outputId": "4736c9e3-058e-44f8-d4f1-6324fb160101"
      },
      "source": [
        "Drop=0.3\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(128,input_shape= (x_train_2.shape[1],)))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(Drop))\n",
        "\n",
        "model.add(Dense(128))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(Drop))\n",
        "\n",
        "model.add(Dense(128))\n",
        "\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(Drop))\n",
        "\n",
        "model.add(Dense(nb_classes))\n",
        "\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "print (model.summary())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 128)               936960    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 5)                 645       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 970,629\n",
            "Trainable params: 970,629\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1YATLMxaKlP",
        "outputId": "c486b7a3-d020-4ab5-c187-307b9d9836b2"
      },
      "source": [
        "model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs,verbose=1,validation_split=0.2 )"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "112/112 [==============================] - 2s 11ms/step - loss: 1.2971 - val_loss: 0.9508\n",
            "Epoch 2/10\n",
            "112/112 [==============================] - 1s 9ms/step - loss: 0.7998 - val_loss: 0.7941\n",
            "Epoch 3/10\n",
            "112/112 [==============================] - 1s 9ms/step - loss: 0.4063 - val_loss: 0.8895\n",
            "Epoch 4/10\n",
            "112/112 [==============================] - 1s 9ms/step - loss: 0.1984 - val_loss: 1.0059\n",
            "Epoch 5/10\n",
            "112/112 [==============================] - 1s 9ms/step - loss: 0.1334 - val_loss: 1.1244\n",
            "Epoch 6/10\n",
            "112/112 [==============================] - 1s 9ms/step - loss: 0.1233 - val_loss: 1.1991\n",
            "Epoch 7/10\n",
            "112/112 [==============================] - 1s 9ms/step - loss: 0.0863 - val_loss: 1.2377\n",
            "Epoch 8/10\n",
            "112/112 [==============================] - 1s 9ms/step - loss: 0.0779 - val_loss: 1.3157\n",
            "Epoch 9/10\n",
            "112/112 [==============================] - 1s 9ms/step - loss: 0.0745 - val_loss: 1.4020\n",
            "Epoch 10/10\n",
            "112/112 [==============================] - 1s 9ms/step - loss: 0.0557 - val_loss: 1.3399\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8d8cb61590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nbJgySZcnlJ",
        "outputId": "12ad18c0-f4f8-470e-c33d-06cd795bf512"
      },
      "source": [
        "y_train_predclass = model.predict_classes(x_train_2,batch_size=batch_size)\n",
        "\n",
        "y_test_predclass = model.predict_classes(x_test_2,batch_size=batch_size)\n",
        "\n",
        "from sklearn.metrics import accuracy_score,classification_report\n",
        "print (\"nnDeep Neural Network - Train accuracy:\"),(round(accuracy_score( y_train, y_train_predclass),3))\n",
        "\n",
        "print (\"nDeep Neural Network - Test accuracy:\"),(round(accuracy_score( y_test,y_test_predclass),3))\n",
        "\n",
        "print (\"nDeep Neural Network - Train Classification Report\")\n",
        "\n",
        "print (classification_report(y_train,y_train_predclass))\n",
        "\n",
        "print (\"nDeep Neural Network - Test Classification Report\")\n",
        "\n",
        "print (classification_report(y_test,y_test_predclass))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "nnDeep Neural Network - Train accuracy:\n",
            "nDeep Neural Network - Test accuracy:\n",
            "nDeep Neural Network - Train Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.87      0.89       904\n",
            "           1       0.95      0.96      0.95      2612\n",
            "           2       0.88      0.89      0.89       569\n",
            "           3       0.97      0.96      0.96       313\n",
            "           4       0.95      0.86      0.90        69\n",
            "\n",
            "    accuracy                           0.93      4467\n",
            "   macro avg       0.93      0.91      0.92      4467\n",
            "weighted avg       0.93      0.93      0.93      4467\n",
            "\n",
            "nDeep Neural Network - Test Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.56      0.57       217\n",
            "           1       0.81      0.82      0.82       677\n",
            "           2       0.49      0.53      0.51       125\n",
            "           3       0.83      0.81      0.82        80\n",
            "           4       0.91      0.56      0.69        18\n",
            "\n",
            "    accuracy                           0.73      1117\n",
            "   macro avg       0.72      0.66      0.68      1117\n",
            "weighted avg       0.74      0.73      0.73      1117\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}