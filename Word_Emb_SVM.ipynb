{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Emb SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1EYuS0TgJ5a3qoqU--HyXnMjFiepBrtcN",
      "authorship_tag": "ABX9TyOEmoDEl0eqOiIfs4XJauGG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asabbah44/Text_Classification/blob/main/Word_Emb_SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7wLnCO6kPzp"
      },
      "source": [
        "https://www.nbshare.io/notebook/197284676/Word-Embeddings-Transformers-In-SVM-Classifier-Using-Python/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLom4QVkbZjh",
        "outputId": "eaf43f8f-6f9a-485e-9753-b31201648a61"
      },
      "source": [
        "!pip install tensorflow_text\n",
        "import tensorflow_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow_text in /usr/local/lib/python3.7/dist-packages (2.4.3)\n",
            "Requirement already satisfied: tensorflow<2.5,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (2.4.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_text) (0.11.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.32.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.7.4.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.3.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.19.5)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.36.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (2.4.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (3.12.4)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (0.12.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.15.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.5,>=2.4.0->tensorflow_text) (1.12.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.28.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (54.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (2020.12.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (4.7.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.8.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow<2.5,>=2.4.0->tensorflow_text) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0mpJYMfOL0p"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text  # this needs to be imported to set up some stuff in the background"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpqFcW1iQSXo"
      },
      "source": [
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UciQHzNibmfX"
      },
      "source": [
        "import re\n",
        "\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l784aFLbppX"
      },
      "source": [
        "import string\n",
        "\n",
        "\n",
        "from spacy.lang.en import stop_words as spacy_stopwords  # we use spacy's list of stop words to clean our data\n",
        "\n",
        "\n",
        "stop_words = spacy_stopwords.STOP_WORDS\n",
        "punctuations = string.punctuation\n",
        "\n",
        "\n",
        "\n",
        "def clean(text):\n",
        "  \n",
        "    text = re.sub(r'\\W+', ' ', text)  # remove non-alphanumeric characters\n",
        "    # replace numbers with the word 'number'\n",
        "    text = re.sub(r\"\\d+\", \"number\", text)\n",
        "    # don't consider sentenced with less than 3 words (i.e. assumed noise)\n",
        "    if len(text.strip().split()) < 3:\n",
        "        return None\n",
        "    text = text.lower()  # lower case everything\n",
        "    \n",
        "    return text.strip() # remove redundant spaces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQOkgniBcDCY"
      },
      "source": [
        "df1 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/dataset.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/mydataset.csv')\n",
        "\n",
        "\n",
        "df1 = pd.DataFrame(df1, columns = ['commenttext','label'])\n",
        "df2 = pd.DataFrame(df2, columns = ['commenttext','label'])\n",
        "\n",
        "df=df2#pd.concat([df1,df2])\n",
        "\n",
        "df = df.assign(clean_text=df.commenttext.apply(clean)).dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7s0ksRQqciCw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIcNT1q0cmyn"
      },
      "source": [
        "# we split the data into train and test\n",
        "msg_train, msg_test, y_train, y_test = train_test_split(df.clean_text, df.label,test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fzyo_1GJdKrZ",
        "outputId": "1ae72258-f22d-487c-847e-55e3fdbea3c3"
      },
      "source": [
        "# we just feed in the list of sentences, and we get the vector representation of each sentence\n",
        "X_test = embed(msg_test)\n",
        "X_test.shape\n",
        "\n",
        "print(msg_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7d8b8bdef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 10 calls to <function recreate_function.<locals>.restored_function_body at 0x7f7d8b8bdef0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1072                 todo check for access modifiers here\n",
            "595     todo come up with an intelligent way to size t...\n",
            "969                            todo create security tests\n",
            "599     todo a better and faster rest pattern matcher ...\n",
            "1289    todo think about threading requirements for wr...\n",
            "                              ...                        \n",
            "179                                     unit test updates\n",
            "865     testnumber src does not exceed quota and dst h...\n",
            "815     todo we need revisit this test when the call b...\n",
            "596     todo move uri params to a mllpconfiguration cl...\n",
            "1060      todo format should come from a preference model\n",
            "Name: clean_text, Length: 302, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKYrY7j0dONP"
      },
      "source": [
        "# we don't have enough memory to apply embeddings in one shot,\n",
        "# so we have to split the data into batches and concatenate them later\n",
        "splits = np.array_split(msg_train, 5)\n",
        "l = list()\n",
        "for split in splits:\n",
        "    l.append(embed(split))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTAWMZikdSZ_",
        "outputId": "244f116e-b44d-477f-89eb-f5934739a5a6"
      },
      "source": [
        "X_train = tf.concat(l, axis=0)\n",
        "del l\n",
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1205, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHSfs6budW2I"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah2tTFF4dYyI",
        "outputId": "9c844caa-31f2-4f9f-bd82-657b5ef99d43"
      },
      "source": [
        "class_weight = compute_class_weight(\n",
        "    class_weight='balanced', classes=[\"Requirement\",\"Design\",\"Defect\",\"Test\",\"Documentation\"], y=y_train\n",
        ")\n",
        "class_weight\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.87318841,  0.53436807,  1.06167401,  1.06167401, 10.04166667])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSPmYYVrlZVz"
      },
      "source": [
        "class_weight={\"Requirement\":class_weight[0], \"Design\":class_weight[1],\"Defect\":class_weight[2],\"Test\":class_weight[3],\n",
        "                        \"Documentation\":class_weight[4]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW0DhyEEd1WQ",
        "outputId": "5381c974-cda8-4314-adba-73bb5835d926"
      },
      "source": [
        "# initialize the model and assign weights to each class\n",
        "clf = SVC()\n",
        "# train the model\n",
        "clf.fit(X_train, y_train)\n",
        "# use the model to predict the testing instances\n",
        "y_pred = clf.predict(np.array(X_test))\n",
        "# generate the classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "       Defect       0.70      0.61      0.65        57\n",
            "       Design       0.70      0.75      0.72       119\n",
            "Documentation       1.00      0.14      0.25         7\n",
            "  Requirement       0.56      0.64      0.59        55\n",
            "         Test       0.98      0.92      0.95        64\n",
            "\n",
            "     accuracy                           0.73       302\n",
            "    macro avg       0.79      0.61      0.63       302\n",
            " weighted avg       0.74      0.73      0.72       302\n",
            "\n",
            "accuracy 0.7251655629139073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnW5_uoLeQ_4",
        "outputId": "a669d8a4-95e4-4142-8174-ad616a1acc39"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(np.array(X_test))\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
        "print('accuracy %s' % accuracy_score(y_pred, y_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "       Defect       0.70      0.53      0.60        57\n",
            "       Design       0.64      0.85      0.73       119\n",
            "Documentation       0.00      0.00      0.00         7\n",
            "  Requirement       0.60      0.47      0.53        55\n",
            "         Test       0.96      0.86      0.91        64\n",
            "\n",
            "     accuracy                           0.70       302\n",
            "    macro avg       0.58      0.54      0.55       302\n",
            " weighted avg       0.70      0.70      0.69       302\n",
            "\n",
            "accuracy 0.7019867549668874\n",
            "accuracy 0.7019867549668874\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zluG5fale7XY",
        "outputId": "ac197a3b-4ce8-4a38-df5f-c9f16a9bd81b"
      },
      "source": [
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(np.array(X_test))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "       Defect       0.64      0.53      0.58        57\n",
            "       Design       0.68      0.76      0.72       119\n",
            "Documentation       1.00      0.14      0.25         7\n",
            "  Requirement       0.56      0.62      0.59        55\n",
            "         Test       0.98      0.92      0.95        64\n",
            "\n",
            "     accuracy                           0.71       302\n",
            "    macro avg       0.77      0.59      0.62       302\n",
            " weighted avg       0.72      0.71      0.71       302\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}